# üè° Pr√©diction des Prix Immobiliers avec PySpark et Gradient Boosted Trees üöÄ

## üìã Description du mod√®le ML choisi

### üéØ Objectif
Le mod√®le vise √† pr√©dire les prix des propri√©t√©s immobili√®res en fonction de diverses caract√©ristiques comme la surface habitable, le nombre de chambres, la localisation et d'autres attributs pertinents. Cette pr√©diction aide les acheteurs, vendeurs et agents immobiliers √† √©valuer pr√©cis√©ment la valeur marchande des biens.

### üå≥ Algorithme
Nous avons utilis√© l'algorithme Gradient Boosted Trees (GBT), une m√©thode d'ensemble puissante qui combine plusieurs arbres de d√©cision pour am√©liorer les performances pr√©dictives. GBT construit s√©quentiellement des arbres de d√©cision qui corrigent les erreurs des arbres pr√©c√©dents, ce qui le rend particuli√®rement efficace pour la mod√©lisation des relations non lin√©aires entre les caract√©ristiques et la variable cible.

### üìä M√©triques d'√©valuation
- **R¬≤** (coefficient de d√©termination) : Mesure la proportion de variance expliqu√©e par le mod√®le
- **RMSE** (Root Mean Square Error) : Quantifie l'erreur moyenne de pr√©diction
- **MAE** (Mean Absolute Error) : Mesure l'erreur absolue moyenne

## üìö Description du dataset utilis√©

### üìè Taille et format
- **Format** : CSV
- **Nombre d'observations** : 8 774 propri√©t√©s
- **Nombre de colonnes** : 59 (58 caract√©ristiques + 1 cible)
- **Taille totale** : ~2.5 Mo

### üß© Structure des donn√©es
Le dataset contient une grande vari√©t√© de colonnes couvrant:

1. **Variables de base**:
   - `price`: Prix de la propri√©t√© (variable cible)
   - `property_type`: Type de propri√©t√©
   - `bedrooms`, `bathrooms`, `total_rooms`: Nombre de pi√®ces
   - `living_area`, `land_area`: Surfaces en m¬≤

2. **Caract√©ristiques binaires** (pr√©sence/absence d'√©quipements):
   - √âquipements de base: climatisation, TV parabolique, parking, garage, etc.
   - √âquipements de confort: piscine, chemin√©e, terrasse, chauffage central, etc.
   - √âquipements de luxe: vue sur mer, ascenseur, syst√®me d'alarme, etc.

3. **Variables d√©riv√©es** (issues du feature engineering):
   - Ratios: `living_land_ratio`, `bed_bath_ratio`, `bath_per_room`
   - Variables logarithmiques: `log_living_area`, `log_land_area`, `log_price`
   - Variables d'interaction: `bed_bath`, `bed_living`, `bath_living`
   - Variables au carr√©: `living_area_squared`, `total_rooms_squared`, etc.
   - Agr√©gations: `amenity_count`, `basic_amenities`, `luxury_amenities`

4. **Variables encod√©es**:
   - `neighborhood_encoded`, `city_encoded`: Variables cat√©gorielles transform√©es

### üåç Source
Le jeu de donn√©es contient des informations sur des propri√©t√©s immobili√®res en Tunisie, collect√©es √† partir de plusieurs sites web sp√©cialis√©s. Les donn√©es ont √©t√© obtenues via un processus de web scraping structur√© :

![Processus de collecte des donn√©es](scrap.png)

1. **Sites sources** : Trois plateformes immobili√®res tunisiennes ont √©t√© utilis√©es (tecnocasa.tn, menzili.tn et mubawab.tn)
2. **M√©thode d'extraction** : Le scraping a √©t√© r√©alis√© avec la biblioth√®que BeautifulSoup en Python
3. **Traitement interm√©diaire** : Les donn√©es ont d'abord √©t√© structur√©es en format JSON
4. **Format final** : Conversion en CSV pour l'analyse avec PySpark

Pour ce projet, nous avons principalement utilis√© les donn√©es issues de **menzili.tn**, qui offrait la meilleure qualit√© et compl√©tude d'information sur les propri√©t√©s, notamment concernant les caract√©ristiques d√©taill√©es, les √©quipements et la localisation pr√©cise des biens.

Le dataset final contient 8 774 propri√©t√©s avec des informations d√©taill√©es sur la surface habitable, le nombre de pi√®ces, l'emplacement g√©ographique, et diverses caract√©ristiques des logements.

## üîÑ D√©tails sur l'adaptation du mod√®le √† PySpark

### üîß D√©fis techniques
L'adaptation d'un mod√®le XGBoost original vers PySpark GBT a n√©cessit√© plusieurs ajustements pour garantir la compatibilit√© avec l'environnement distribu√©:

1. **Transformation des donn√©es** : Utilisation des StringIndexer pour convertir les variables cat√©gorielles en valeurs num√©riques
2. **Cr√©ation d'un pipeline** : Mise en place d'un workflow structur√© incluant pr√©traitement, assemblage des caract√©ristiques et entra√Ænement
3. **Optimisation des hyperparam√®tres** : Ajustement des param√®tres GBT pour √©quilibrer vitesse d'ex√©cution et performance
4. **Gestion des valeurs aberrantes** : Filtrage des prix extr√™mes pour am√©liorer la robustesse du mod√®le

### üîÑ Pipeline Spark impl√©ment√©

![Pipeline Spark pour la Pr√©diction des Prix Immobiliers](pipeline_spark.png)

Le pipeline Spark impl√©ment√© comprend 7 √©tapes s√©quentielles:

1. **Sources de Donn√©es**: Chargement parall√®le depuis HDFS ou syst√®me de fichiers local
2. **Pr√©traitement**: Nettoyage, gestion des valeurs manquantes, filtrage des outliers
3. **Feature Engineering**: Cr√©ation de variables d√©riv√©es, ratios et interactions
4. **Indexation**: Conversion des variables cat√©gorielles en valeurs num√©riques
5. **Assemblage de Vecteurs**: Regroupement des caract√©ristiques pour le mod√®le
6. **Entra√Ænement**: Application de l'algorithme GBT avec param√®tres optimis√©s
7. **√âvaluation**: Calcul des m√©triques de performance sur les donn√©es de test

```python
# Cr√©ation des indexeurs pour colonnes cat√©gorielles
indexers = [StringIndexer(inputCol=col_name, outputCol=col_name + "_indexed", 
                         handleInvalid="keep") 
           for col_name in string_cols]

# Assembleur de vecteurs
assembler = VectorAssembler(
    inputCols=feature_cols + [c + "_indexed" for c in string_cols], 
    outputCol="features", 
    handleInvalid="skip"
)

# R√©gression GBT optimis√©e
gbt_regressor = GBTRegressor(
    featuresCol="features", 
    labelCol=log_target_col,
    maxIter=150,         
    maxDepth=7,          
    stepSize=0.03,       
    subsamplingRate=0.7,  
    featureSubsetStrategy="all",
    maxBins=40,          
    minInstancesPerNode=1,
    seed=42               
)

# Pipeline complet
pipeline = Pipeline(stages=indexers + [assembler, gbt_regressor])
```

### üöÄ Am√©liorations apport√©es
Pour optimiser les performances du mod√®le, nous avons impl√©ment√© plusieurs am√©liorations:

1. **Feature Engineering avanc√©**:
   - Cr√©ation de termes quadratiques pour les variables importantes
   - Ajout de ratios (surface habitable/terrain, chambres/salles de bain)
   - Cr√©ation de termes d'interaction entre variables cl√©s

2. **Optimisation des param√®tres du mod√®le**:
   - Augmentation du nombre d'it√©rations (150 vs 100)
   - Augmentation de la profondeur des arbres (7 vs 5)
   - R√©duction du taux d'apprentissage (0.03 vs 0.05)
   - Utilisation de toutes les caract√©ristiques (au lieu d'un sous-ensemble)

3. **Traitement des valeurs aberrantes**:
   - Filtrage des prix extr√™mes (¬±3 √©carts-types)
   - Gestion appropri√©e des valeurs manquantes

## üìà R√©sultats obtenus et comparaison avec le mod√®le original

### üìä Mod√®le initial (GBT simple)
- **R¬≤ (√©chelle log)** : 0.6855
- **RMSE (√©chelle log)** : 0.3928
- **MAE (√©chelle log)** : 0.2949
- **R¬≤ (√©chelle originale)** : 0.6614
- **RMSE (√©chelle originale)** : 182 213.6692

### üìà Mod√®le am√©lior√© (GBT optimis√© avec feature engineering)
- **R¬≤ (√©chelle log)** : 0.6918
- **RMSE (√©chelle log)** : 0.3889
- **MAE (√©chelle log)** : 0.2845
- **R¬≤ (√©chelle originale)** : 0.6635
- **RMSE (√©chelle originale)** : 181 638.4601
- **R¬≤ ajust√©** : 0.6788

### üîç Comparaison d√©taill√©e avec XGBoost
Notre projet visait √† adapter un mod√®le XGBoost existant vers l'environnement Spark. Voici les performances exactes du mod√®le XGBoost original:

```
Training XGBoost...
Cross-validation R¬≤ scores: [0.74317699 0.74216383 0.7464795 0.73447899 0.76193954]
Mean CV R¬≤ score: 0.7456
Test Results for XGBoost:
Log Scale: R¬≤ = 0.7560, RMSE = 0.3497, MAE = 0.2527
Original Scale: R¬≤ = 0.7390, RMSE = 157288.7441
```

Notre impl√©mentation PySpark avec GBT a atteint un R¬≤ de 0.6918 sur l'√©chelle logarithmique, repr√©sentant 91.5% des performances du mod√®le XGBoost original (R¬≤ = 0.7560). Sur l'√©chelle originale, notre mod√®le a atteint environ 89.8% des performances XGBoost (0.6635 vs 0.7390).

Cette diff√©rence de performance peut s'expliquer par plusieurs facteurs:

1. **Diff√©rences algorithmiques**: XGBoost et GBT de Spark pr√©sentent des impl√©mentations distinctes des arbres boost√©s, avec XGBoost incorporant des r√©gularisations sp√©cifiques
2. **Optimisations distribu√©es**: Le GBT de Spark est con√ßu pour le traitement distribu√©, impliquant certains compromis pour la scalabilit√©
3. **Hyperparam√®tres disponibles**: XGBoost dispose d'un ensemble plus riche d'hyperparam√®tres permettant un fine-tuning plus pr√©cis
4. **Gestion des valeurs cat√©gorielles**: XGBoost et GBT diff√®rent dans leur traitement des variables cat√©gorielles

Malgr√© cette diff√©rence de performance, notre impl√©mentation offre l'avantage consid√©rable du passage √† l'√©chelle sur de grands volumes de donn√©es, d√©montrant ainsi la viabilit√© de l'adaptation d'algorithmes complexes au contexte Big Data.

## üõ†Ô∏è Configuration de l'environnement Big Data

### üê≥ Modification du docker-compose.yml

Pour adapter notre environnement Docker √† l'ex√©cution de t√¢ches d'apprentissage automatique distribu√©es, nous avons apport√© plusieurs modifications au fichier docker-compose.yml:

```yaml
version: '3'

services:
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    volumes:
      - hadoop_namenode:/hadoop/dfs/name
      - ./data:/data
    environment:
      - CLUSTER_NAME=hadoop-cluster
    env_file:
      - ./hadoop.env
    ports:
      - "9870:9870"
      - "8020:8020"
    networks:
      - hadoop-network

  spark-master:
    image: custom-spark:latest  # Image personnalis√©e avec les packages ML
    container_name: spark-master
    depends_on:
      - namenode
    ports:
      - "8080:8080"
      - "7077:7077"
      - "4040:4040"
    volumes:
      - ./scripts:/scripts
      - ./data:/tmp/data
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    networks:
      - hadoop-network

  spark-worker:
    image: custom-spark:latest  # Image personnalis√©e
    container_name: spark-worker
    depends_on:
      - spark-master
    ports:
      - "8081:8081"
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=2G
      - SPARK_WORKER_CORES=2
    volumes:
      - ./data:/tmp/data
    networks:
      - hadoop-network

networks:
  hadoop-network:
    driver: bridge

volumes:
  hadoop_namenode:
```

Les principales modifications incluent:
- Utilisation d'une image personnalis√©e `custom-spark` avec les d√©pendances n√©cessaires
- Montage de volumes pour faciliter le transfert de donn√©es (scripts et datasets)
- Configuration des ressources pour les workers (m√©moire, c≈ìurs)
- Mise en place d'un r√©seau partag√© entre Hadoop et Spark

### üê≥ Modification du Dockerfile

Nous avons cr√©√© un Dockerfile personnalis√© pour int√©grer les biblioth√®ques n√©cessaires au machine learning:

```dockerfile
FROM bitnami/spark:3.3.0

USER root

# Installation des packages Python essentiels
RUN pip install numpy pandas scikit-learn

# Installation de d√©pendances sp√©cifiques √† notre projet
RUN pip install findspark

# Configuration des variables d'environnement
ENV PYSPARK_PYTHON=/usr/bin/python3
ENV PYSPARK_DRIVER_PYTHON=/usr/bin/python3

# Retour √† l'utilisateur non-root pour la s√©curit√©
USER 1001
```

Cette configuration nous a permis de cr√©er un environnement complet et autonome pour l'ex√©cution de notre pipeline d'apprentissage automatique distribu√©, d√©montrant ainsi la possibilit√© d'adapter des mod√®les complexes comme les arbres boost√©s √† un contexte Big Data.

## üîç Interpr√©tation des r√©sultats

### üìä Performance du mod√®le
Le R¬≤ de 0.6635 sur l'√©chelle originale indique que notre mod√®le explique environ 66% de la variance des prix immobiliers, ce qui est une performance notable pour ce type de donn√©es. Les prix immobiliers sont notoirement difficiles √† pr√©dire en raison de facteurs subjectifs (esth√©tique, sentiment) et externes (d√©veloppements urbains, tendances du march√©) qui ne sont pas captur√©s dans les donn√©es.

### üìà Impact des am√©liorations
L'am√©lioration du R¬≤ de 0.6614 √† 0.6635 peut sembler modeste, mais repr√©sente un gain significatif consid√©rant la difficult√© d'am√©liorer un mod√®le d√©j√† performant. Les caract√©ristiques d√©riv√©es et les ratios cr√©√©s se sont r√©v√©l√©s tr√®s influents, confirmant l'importance du feature engineering avanc√©.

### üîë Facteurs d√©terminants du prix
L'analyse de l'importance des caract√©ristiques r√©v√®le que:
- La localisation (quartier, ville) reste le facteur le plus d√©terminant du prix
- Les ratios et interactions (surface par chambre, ratio surface habitable/terrain) sont plus pr√©dictifs que les mesures brutes
- Les caract√©ristiques qualitatives encod√©es ont un impact significatif sur le prix final

### üíº Applications pratiques
Ce mod√®le peut √™tre utilis√© pour:
- Estimer la valeur marchande des propri√©t√©s
- Identifier les caract√©ristiques qui maximisent la valeur d'un bien
- D√©tecter les propri√©t√©s sous-√©valu√©es ou sur√©valu√©es sur le march√©
- Analyser les tendances de prix par r√©gion

## üèÅ Conclusion

L'adaptation du mod√®le de pr√©diction des prix immobiliers √† PySpark a √©t√© r√©alis√©e avec succ√®s, atteignant pr√®s de 90% des performances du mod√®le XGBoost original tout en b√©n√©ficiant des avantages du traitement distribu√©. Les techniques d'ing√©nierie de caract√©ristiques et l'optimisation des param√®tres ont permis d'am√©liorer les performances, d√©montrant l'efficacit√© de l'approche utilis√©e.

Le mod√®le final pr√©sente un excellent √©quilibre entre pr√©cision pr√©dictive et capacit√© de passage √† l'√©chelle, ce qui le rend appropri√© pour des applications immobili√®res dans un contexte Big Data.