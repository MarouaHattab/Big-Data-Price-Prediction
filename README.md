# ğŸ¡ PrÃ©diction des Prix Immobiliers avec PySpark ğŸš€

Ce projet implÃ©mente un modÃ¨le de prÃ©diction des prix immobiliers utilisant PySpark et l'algorithme Gradient Boosted Trees (GBT). Il dÃ©montre l'adaptation d'un modÃ¨le XGBoost vers l'Ã©cosystÃ¨me Spark pour permettre le traitement distribuÃ© et le passage Ã  l'Ã©chelle.

## ğŸ“‹ AperÃ§u du Projet

![Pipeline Spark pour la PrÃ©diction des Prix Immobiliers](pipeline_spark.png)

Le projet comprend:
- Un modÃ¨le de prÃ©diction des prix immobiliers basÃ© sur PySpark ML
- Un pipeline Spark complet pour le prÃ©traitement, le feature engineering et l'entraÃ®nement
- Des amÃ©liorations techniques pour maximiser la prÃ©cision (RÂ² = 0.6918)
- Une architecture distribuÃ©e pour le traitement de grands volumes de donnÃ©es

## ğŸ”§ Structure du Projet

```
â”œâ”€â”€ docker-compose.yml          # Configuration de l'environnement distribuÃ©
â”œâ”€â”€ Dockerfile                  # Image personnalisÃ©e avec les dÃ©pendances ML
â”œâ”€â”€ scripts/                    # Scripts d'exÃ©cution et d'analyse
â”‚   â”œâ”€â”€ spark_gbt_basic.py      # ImplÃ©mentation GBT simple (RÂ² = 0.6855)
â”‚   â””â”€â”€ spark_gbt_optimized.py  # ImplÃ©mentation GBT optimisÃ©e (RÂ² = 0.6918)
â”œâ”€â”€ data/                       # DonnÃ©es immobiliÃ¨res
â”‚   â””â”€â”€ data.csv                # Dataset des propriÃ©tÃ©s (8774 observations)
â”œâ”€â”€ rapport.md                  # Rapport dÃ©taillÃ© du projet
â””â”€â”€ README.md                   # Ce fichier
```

## ğŸŒŸ CaractÃ©ristiques Principales

- **Feature Engineering AvancÃ©**: CrÃ©ation de variables quadratiques, ratios et interactions
- **Optimisation des HyperparamÃ¨tres**: Configuration fine du GBT pour maximiser le RÂ²
- **Traitement des Valeurs Aberrantes**: Filtrage intelligents des prix extrÃªmes
- **Pipeline Spark Complet**: Workflow reproductible de bout en bout
- **Environnement ContainerisÃ©**: Configuration Docker prÃªte Ã  l'emploi

## ğŸ“Š RÃ©sultats

| ModÃ¨le              | RÂ² (Ã©chelle log) | RÂ² (Ã©chelle originale) | RMSE             |
|---------------------|------------------|------------------------|------------------|
| GBT Simple          | 0.6855           | 0.6614                 | 182 213.6692     |
| GBT OptimisÃ©        | 0.6918           | 0.6635                 | 181 638.4601     |
| XGBoost (rÃ©fÃ©rence) | 0.7560           | 0.7390                 | 157 288.7441     |

Le modÃ¨le GBT optimisÃ© atteint 91.5% des performances du modÃ¨le XGBoost original (Ã©chelle log) et 89.8% sur l'Ã©chelle originale, tout en offrant les avantages du traitement distribuÃ©.

## ğŸš€ Installation et ExÃ©cution

### PrÃ©requis
- Docker & Docker Compose
- Git

### Configuration

1. Cloner le dÃ©pÃ´t:
```bash
git clone https://github.com/MarouaHattab/Big-Data-Price-Prediction
cd prediction-prix-immobiliers
```

2. Construire l'image Docker personnalisÃ©e:
```bash
docker build -t custom-spark:latest .
```

3. Lancer l'environnement distribuÃ©:
```bash
docker-compose up -d
```

### PrÃ©paration des donnÃ©es dans HDFS

1. CrÃ©er le rÃ©pertoire de projet dans HDFS:
```bash
docker exec -it namenode hdfs dfs -mkdir -p /ProjetBigData
```

2. Copier le dataset CSV vers le container namenode:
```bash
docker cp ./data/data.csv namenode:/tmp/
```

3. Copier le dataset depuis le container vers HDFS:
```bash
docker exec -it namenode hdfs dfs -put /tmp/data.csv /ProjetBigData/
```

4. VÃ©rifier que le fichier est bien copiÃ©:
```bash
docker exec -it namenode hdfs dfs -ls /ProjetBigData/
```

### PrÃ©paration du script Python

1. Copier le script Python optimisÃ© vers le container Spark:
```bash
docker cp ./scripts/spark_gbt_optimized.py spark-master:/tmp/spark.py
```

2. VÃ©rifier que le script est bien copiÃ©:
```bash
docker exec -it spark-master ls -la /tmp/
```

### ExÃ©cution du pipeline

```bash
docker exec -it spark-master /opt/bitnami/spark/bin/spark-submit --master local[*] --conf spark.executor.memory=2g --conf spark.driver.memory=2g /tmp/spark.py
```

### Suivre l'exÃ©cution

Vous pouvez suivre l'exÃ©cution du job via l'interface web Spark:
- Spark UI: http://localhost:8080
- Application UI: http://localhost:4040 (pendant l'exÃ©cution)
- HDFS UI: http://localhost:9870

### RÃ©cupÃ©rer les rÃ©sultats

Si votre script Ã©crit des rÃ©sultats dans HDFS:
```bash
# CrÃ©er le rÃ©pertoire pour les rÃ©sultats
docker exec -it namenode hdfs dfs -mkdir -p /ProjetBigData/results

# Copier les rÃ©sultats depuis HDFS
docker exec -it namenode hdfs dfs -get /ProjetBigData/results ./results
```

## ğŸ“ˆ Visualisations et MÃ©triques

Les principales caractÃ©ristiques prÃ©dictives sont:
- `neighborhood_encoded` (0.1177) - L'emplacement reste roi!
- `bath_living` (0.0879) - L'interaction surface/salle de bain
- `city_encoded` (0.0681) - La ville oÃ¹ se trouve la propriÃ©tÃ©
- `area_per_bedroom` (0.0603) - La surface par chambre
- `living_land_ratio` (0.0526) - Le ratio bÃ¢ti/terrain

## ğŸ§© Adaptation Ã  d'autres datasets

Le pipeline est facilement adaptable Ã  d'autres jeux de donnÃ©es immobiliers:

1. Remplacer le fichier CSV dans le dossier data/
```bash
# Copier un nouveau dataset
cp /chemin/vers/nouveau/dataset.csv ./data/

# Mettre Ã  jour HDFS
docker exec -it namenode hdfs dfs -rm /ProjetBigData/data.csv
docker cp ./data/dataset.csv namenode:/tmp/
docker exec -it namenode hdfs dfs -put /tmp/dataset.csv /ProjetBigData/data.csv
```

2. Ajuster les noms de colonnes dans le script si nÃ©cessaire
```bash
# Modifier le script
nano ./scripts/spark_gbt_optimized.py

# Mettre Ã  jour le script dans le container
docker cp ./scripts/spark_gbt_optimized.py spark-master:/tmp/spark.py
```
![capture ](copy.png)
3. Relancer l'exÃ©cution avec spark-submit
```bash
docker exec -it spark-master /opt/bitnami/spark/bin/spark-submit --master local[*] /tmp/spark.py
```
![capture ](modeling.png)

## ğŸ“œ Rapport Complet
Le rapport dÃ©taillÃ© du projet est disponible [ici](rapport.md). Il couvre les Ã©tapes de prÃ©traitement, le feature engineering, l'entraÃ®nement du modÃ¨le et l'Ã©valuation des performances.
## ğŸ“š Ressources
- [Documentation PySpark](https://spark.apache.org/docs/latest/api/python/index.html)
- [Documentation Spark ML](https://spark.apache.org/docs/latest/ml-guide.html)
- [Documentation Docker](https://docs.docker.com/)
- [Documentation Docker Compose](https://docs.docker.com/compose/)
- [Documentation HDFS](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsCommands.html)
- [Documentation XGBoost](https://xgboost.readthedocs.io/en/latest/)
- [Documentation Gradient Boosted Trees](https://spark.apache.org/docs/latest/ml-classification-regression.html#gradient-boosted-trees-gbt)
- [Documentation Feature Engineering](https://spark.apache.org/docs/latest/ml-features.html)
- [Documentation Feature Importance](https://spark.apache.org/docs/latest/api/python/reference/pyspark.ml.html#pyspark.ml.feature.FeatureImportance)
- [Documentation Hyperparameter Tuning](https://spark.apache.org/docs/latest/api/python/reference/pyspark.ml.html#pyspark.ml.tuning.HyperparameterTuning)

## ğŸ‘¨â€ğŸ’» Contributeurs

- [Maroua HATTAB](https://github.com/MarouaHattab)

